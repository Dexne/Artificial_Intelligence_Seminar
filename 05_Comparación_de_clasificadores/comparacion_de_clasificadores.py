# -*- coding: utf-8 -*-
"""Comparacion_De_Clasificadores.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oYnd3je5XUYq897RnFfv-Gpzjom8pGSo
"""

# Comparaci√≥n de clasificadores
# Importamos los modulos y librerias

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB

# Clasificadores
classifiers = {'KNN': KNeighborsClassifier(3),
               'SVM': SVC(gamma=2, C=1),
               'GP': GaussianProcessClassifier(1.0 *RBF(1.0)),
               'DT': DecisionTreeClassifier(max_depth=5),
               'MLP': MLPClassifier(alpha=0.1, max_iter=1000),
               'Bayes': GaussianNB()}

x, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           n_clusters_per_class=1)
rng = np.random.RandomState(2)
x += 1 * rng.uniform(size=x.shape)
linearly_separable = (x, y)

datasets = [make_moons(noise=0.1),
            make_circles(noise=0.1, factor=0.5),
            linearly_separable ]

cm = plt.cm.RdBu
cm_bright = ListedColormap(['#FF0000', '#0000FF'])

model_name = 'Bayes'

figure = plt.figure(figsize=(9,3))
h = .02 # step
i = 1 # counter

# iterate over datasets
for ds_cnt, ds in enumerate(datasets):

  # Escalamos
  x, y = ds
  x = StandardScaler(). fit_transform(x)
  # hacemos el train y test
  xtrain, xtest, ytrain, ytest = train_test_split(x,y)
  # sacamos limites
  x_min, x_max = x[:,0].min() - .5, x[:,0].max() + .5
  y_min, y_max = x[:,1].min() - .5, x[:,1].max() + .5
  xx,yy = np.meshgrid(np.arange(x_min, x_max, h),
                      np.arange(y_min, y_max, h))
  # clasificador
  model = classifiers[model_name]
  ax = plt.subplot(1,3,i)
  # Entrenamos
  model.fit(xtrain, ytrain)
  # sacamos las metricas
  score_train = model.score(xtrain, ytrain)
  score_test = model.score(xtest, ytest)

  # Plot the decision boundary. For that, we will asssign a color to each
  #point in the mesh [x_min, x_max] x[y_min, y_max].
  if hasattr(model, "decision_function"):
    zz = model.decision_function(np.c_[xx.ravel(), yy.ravel()])
  else:
    zz = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]

  # Put the resuslt into a color plot
  zz = zz.reshape(xx.shape)
  ax.contourf(xx, yy, zz, cmap=cm, alpha=.8)

  # plot the training point
  ax.scatter(xtest[:,0], xtest[:,1], c=ytest, cmap=cm_bright,
             edgecolors='k', alpha=0.6)

  ax.set_xlim(xx.min(), xx.max())
  ax.set_ylim(yy.min(), yy.max())
  ax.set_xticks(())
  ax.set_yticks(())

  ax.text(xx.max() - .3, yy.min() + .7, '%.2f' % score_train,
          size=15, horizontalalignment='right')
  ax.text(xx.max() - .3, yy.min() + .3, '%.2f' % score_test,
          size=15, horizontalalignment='right')
  i += 1

plt.tight_layout()
plt.show()